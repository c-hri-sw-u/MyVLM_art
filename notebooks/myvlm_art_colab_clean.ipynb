{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¨ MyVLM-Art è®­ç»ƒ (Clean Version)\n",
        "\n",
        "âš ï¸ **è¦æ±‚**: GPU è¿è¡Œæ—¶ (T4 æˆ–æ›´é«˜)\n",
        "\n",
        "**è¿è¡Œé¡ºåº**:\n",
        "1. è¿è¡Œ Cell 1 (å®‰è£…ä¾èµ–)\n",
        "2. è¿è¡Œ Cell 2 (é‡å¯ - ä¼šè‡ªåŠ¨é‡å¯)\n",
        "3. **é‡å¯å** ä» Cell 3 å¼€å§‹è¿è¡Œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: å®‰è£…ä¾èµ–\n",
        "\n",
        "âš ï¸ è¿è¡Œå®Œåå¿…é¡»é‡å¯ Runtimeï¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# å®Œå…¨å¸è½½æ—§ç‰ˆæœ¬\n",
        "!pip uninstall -y bitsandbytes transformers accelerate -q\n",
        "\n",
        "# å®‰è£…å›ºå®šç‰ˆæœ¬\n",
        "!pip install transformers==4.36.2 --no-cache-dir -q\n",
        "!pip install accelerate==0.25.0 --no-cache-dir -q\n",
        "!pip install bitsandbytes==0.41.3 --no-cache-dir -q\n",
        "!pip install peft>=0.2.0 -q\n",
        "!pip install open-clip-torch -q\n",
        "!pip install timm>=0.9.16 -q\n",
        "!pip install sentence-transformers>=2.3.1 -q\n",
        "!pip install omegaconf>=2.3.0 -q\n",
        "!pip install pyrallis>=0.3.1 -q\n",
        "!pip install loguru>=0.7.0 -q\n",
        "!pip install wandb>=0.16.0 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ\")\n",
        "print(\"âš ï¸ ç°åœ¨è¿è¡Œä¸‹ä¸€ä¸ª Cell é‡å¯ Runtime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: é‡å¯ Runtime\n",
        "\n",
        "âš ï¸ **è¿è¡Œåè‡ªåŠ¨é‡å¯ï¼Œé‡å¯åè·³è¿‡ Cell 1-2ï¼Œç›´æ¥ä» Cell 3 å¼€å§‹**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# â¬‡ï¸â¬‡ï¸â¬‡ï¸ é‡å¯åä»è¿™é‡Œå¼€å§‹ â¬‡ï¸â¬‡ï¸â¬‡ï¸\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: éªŒè¯ç¯å¢ƒ + Clone ä»£ç "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# éªŒè¯ç‰ˆæœ¬\n",
        "import torch\n",
        "import transformers\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Transformers: {transformers.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# æ£€æŸ¥ç‰ˆæœ¬\n",
        "assert transformers.__version__.startswith(\"4.36\"), f\"transformers ç‰ˆæœ¬åº”ä¸º 4.36.xï¼Œå½“å‰: {transformers.__version__}\"\n",
        "print(\"\\nâœ… ç¯å¢ƒéªŒè¯é€šè¿‡\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone ä»£ç \n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# TODO: æ›¿æ¢ä¸ºä½ çš„ä»“åº“åœ°å€\n",
        "REPO_URL = \"https://github.com/YOUR_USERNAME/MyVLM_art.git\"\n",
        "BRANCH = \"log-for-colab\"\n",
        "\n",
        "if os.path.exists(\"/content/MyVLM_art\"):\n",
        "    shutil.rmtree(\"/content/MyVLM_art\")\n",
        "    print(\"ğŸ—‘ï¸ åˆ é™¤æ—§ä»£ç \")\n",
        "\n",
        "!git clone -b {BRANCH} {REPO_URL}\n",
        "os.chdir(\"/content/MyVLM_art\")\n",
        "print(f\"\\nğŸ“ å½“å‰ç›®å½•: {os.getcwd()}\")\n",
        "!git log --oneline -3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Wandb ç™»å½• (å¯é€‰)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "print(\"âœ… Wandb ç™»å½•æˆåŠŸ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: è¿è¡Œè®­ç»ƒ ğŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== å®Œæ•´è®­ç»ƒä»£ç  ====================\n",
        "import sys\n",
        "import os\n",
        "os.chdir(\"/content/MyVLM_art\")\n",
        "sys.path.insert(0, \"/content/MyVLM_art\")\n",
        "\n",
        "# å¯¼å…¥å¹¶é…ç½®\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from myvlm.common import VLMType, PersonalizationTask, VLM_TO_LAYER\n",
        "from configs.myvlm_art_config import MyVLMArtConfig\n",
        "from vlms.llava_wrapper import LLaVAWrapper\n",
        "from myvlm.myllava import MyLLaVA\n",
        "from concept_graph.concept_embeddings.trainer import MultiTokenEmbeddingTrainer\n",
        "\n",
        "# é…ç½®å‚æ•°\n",
        "cfg = MyVLMArtConfig(\n",
        "    concept_name=\"dataset\",\n",
        "    concept_identifier=\"test\",\n",
        "    vlm_type=VLMType.LLAVA,\n",
        "    personalization_task=PersonalizationTask.CAPTIONING,\n",
        "    output_root=Path(\"./outputs\"),\n",
        "    data_root=Path(\"./data\"),\n",
        "    optimization_steps=1,         # å¿«é€Ÿæµ‹è¯•ï¼Œæ­£å¼è®­ç»ƒæ”¹ä¸º 100\n",
        "    learning_rate=1.0,\n",
        "    batch_size=1,                 # T4 å»ºè®® 1-2\n",
        "    reg_lambda=0.0,\n",
        "    device='cuda',\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    threshold=0.75,\n",
        "    max_tokens_per_concept=1,\n",
        "    max_concepts_per_sample=1,\n",
        "    backoff_delta=0.05,\n",
        "    val_subset_n=3,\n",
        "    max_reason_tokens=64,\n",
        "    # Wandb (è®¾ä¸º False ç¦ç”¨)\n",
        "    use_wandb=True,\n",
        "    wandb_project=\"myvlm-art-test\",\n",
        "    wandb_run_name=\"quick-test\",\n",
        ")\n",
        "\n",
        "# è®­ç»ƒ\n",
        "print(\"ğŸ”„ åŠ è½½ LLaVA æ¨¡å‹...\")\n",
        "vlm = LLaVAWrapper(device=cfg.device, torch_dtype=cfg.torch_dtype)\n",
        "\n",
        "print(\"ğŸ”§ åˆå§‹åŒ– MyLLaVA...\")\n",
        "myvlm = MyLLaVA(vlm, layer=VLM_TO_LAYER[cfg.vlm_type], concept_name=cfg.concept_name, cfg=cfg)\n",
        "\n",
        "print(\"ğŸš€ å¼€å§‹è®­ç»ƒ...\")\n",
        "trainer = MultiTokenEmbeddingTrainer(cfg=cfg, myvlm=myvlm, dataset_builder=None)\n",
        "checkpoints = trainer.train()\n",
        "\n",
        "# ä¿å­˜\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "out_dir = cfg.output_root / cfg.concept_name / f\"seed_{cfg.seed}\"\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "ts = datetime.now(ZoneInfo('America/New_York')).strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_path = out_dir / f\"checkpoints_{cfg.concept_name}_seed_{cfg.seed}_{ts}.pt\"\n",
        "torch.save(checkpoints, save_path)\n",
        "\n",
        "print(f\"\\nâœ… è®­ç»ƒå®Œæˆ!\")\n",
        "print(f\"ğŸ’¾ æ£€æŸ¥ç‚¹: {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: ä¿å­˜åˆ° Google Drive (å¯é€‰)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_dir = Path(\"/content/drive/MyDrive/MyVLM_art_outputs\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "shutil.copytree(\"/content/MyVLM_art/outputs\", save_dir / \"outputs\", dirs_exist_ok=True)\n",
        "print(f\"âœ… å·²ä¿å­˜åˆ°: {save_dir / 'outputs'}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
